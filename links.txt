https://github.com/openai/whisper/discussions/104



After digging a bit more on this subject, it feels that timed speaker diarization is only achievable using a hybrid approach.

Using Pyannote (see Majdoddin's work) seems to be a good and fast solution, but adding silences to audio files that will be later fed to Whisper might generate unwanted hallucinations and influence the context of the transcription, especially for non-english transcripts.

jongwook suggested in another discussion to "do a crude form of speaker turn tracking" by using the prompt attribute in the transcription call and inserting a sort of dialogue example to make the model follow the lead (e.g. " - Hey how are you doing? - I'm doing good. How are you?")

Possible approach:
first transcribe the original audio using Whisper with initial_prompt="- Hey how are you doing? - I'm doing good. How are you?")
use pyannote on the same original audio to detect the speakers
use pyannote's results (timings) to check the segments generated by whisper and confirm the speakers
then finally provide a report with the pyannote timings that weren't paired with any whisper segments










https://github.com/facebookresearch/svoice


https://github.com/m-bain/whisperX